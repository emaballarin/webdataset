{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and helper functions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import webdataset as wds\n",
    "import braceexpand\n",
    "import tempfile\n",
    "import glob\n",
    "from itertools import islice\n",
    "import random\n",
    "\n",
    "def summarize(sample):\n",
    "    for k, v in sample.items():\n",
    "        print(k, repr(v)[:100])\n",
    "\n",
    "def read_binary(fname):\n",
    "    with open(fname, \"rb\") as stream:\n",
    "        return stream.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing of Shards: Large Scale OCR\n",
    "\n",
    "This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using `pdftoppm` and `tessearact` into a new dataset consisting of page images and corresponding OCR output.\n",
    "\n",
    "The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like:\n",
    "\n",
    "```Python\n",
    "with WebDataset(srcname) as src:\n",
    "    with TarWriter(dstname) as dst:\n",
    "        for sample in src:\n",
    "            ... do something with sample ...\n",
    "            dst.write(sample)\n",
    "upload(dstname)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Arxiv Dataset of PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808.00020v6.pdf\n",
      "1511.05082v1.pdf\n",
      "1610.08000v1.pdf\n",
      "1506.03736v2.pdf\n",
      "1909.03824v1.pdf\n",
      "tar: stdout: write error\n"
     ]
    }
   ],
   "source": [
    "# The dataset is tar files containing PDFs, each using the Arxiv naming convention.\n",
    "\n",
    "!gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__key__ '1808_00020v6'\n",
      "__url__ 'gs://webdataset/testdata/arxiv-pdfs-000000.tar'\n",
      "pdf b'%PDF-1.5\\n%\\x8f\\n18 0 obj\\n<< /Filter /FlateDecode /Length 5428 >>\\nstream\\nx\\xda\\xad[]\\xb3\\xe3\\xb\n"
     ]
    }
   ],
   "source": [
    "# Arxiv naming convenitions are incompatible with WebDataset, but we can add\n",
    "# a file renaming function to the WebDataset to fix this.\n",
    "\n",
    "def arxiv_rename(name):\n",
    "    return name.replace(\".pdf\", \"\").replace(\".\", \"_\") + \".pdf\"\n",
    "\n",
    "# For this example, we just use two shards, but usually, you would have hundreds\n",
    "# or thousands of shards.\n",
    "\n",
    "dataset = \"gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar\"\n",
    "\n",
    "# Let's open the dataset and read the first sample.\n",
    "\n",
    "shardurls = list(braceexpand.braceexpand(dataset))\n",
    "ds = wds.WebDataset(shardurls, rename_files=arxiv_rename)\n",
    "sample = next(iter(ds))\n",
    "summarize(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Tesseract on a Single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, maxpages=9999, shuffle=True):\n",
    "    \"\"\"Process a sample from the Arxiv dataset.\n",
    "\n",
    "    This function converts the PDF file to a sequence of JPEG images\n",
    "    and then invokes Tesseract to recognize the text in the images.\n",
    "    It returns a sequence of samples, one per page, each containing\n",
    "    the JPEG image and the hOCR output from Tesseract.\n",
    "    \"\"\"\n",
    "\n",
    "    # We work in a temporary directory; most operations are command line tools\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as dirname:\n",
    "\n",
    "        # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm\n",
    "        pdfpath = dirname + \"/sample.pdf\"\n",
    "        with open(pdfpath, \"wb\") as stream:\n",
    "            stream.write(sample[\"pdf\"])\n",
    "        assert os.system(f\"(cd {dirname} && pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)\") == 0\n",
    "        \n",
    "        # Next, we are going to iterate over the pages, convert them to text using tesseract,\n",
    "        pages = sorted(glob.glob(dirname + \"/page-*.jpg\"))\n",
    "        if shuffle:\n",
    "            random.shuffle(pages)\n",
    "\n",
    "        for page in islice(pages, maxpages):\n",
    "            page_without_suffix = page[:-4]\n",
    "            base = os.path.basename(page_without_suffix)\n",
    "\n",
    "            # Invoke Tesseract to convert the page image to hOCR.\n",
    "            os.system(f\"tesseract {page} {page_without_suffix} hocr\")\n",
    "\n",
    "            # Construct the output sample.\n",
    "            nsample = {\n",
    "                \"__key__\": sample[\"__key__\"] + f\"/{base}\",\n",
    "                \"jpg\": read_binary(page_without_suffix + \".jpg\"),\n",
    "                \"hocr\": read_binary(page_without_suffix + \".hocr\"),\n",
    "            }\n",
    "\n",
    "            # This function returns an iterator over the recognized pages.\n",
    "            yield nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__key__ '1808_00020v6/page-19'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x0\n",
      "hocr b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional/\n"
     ]
    }
   ],
   "source": [
    "output = next(process_sample(sample))\n",
    "summarize(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a Shard of PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard(src, dst, maxpdfs=999999, maxpages=9999):\n",
    "    \"\"\"Process a shard of the Arxiv dataset.\n",
    "\n",
    "    This function reads a shard of the Arxiv dataset, processes each sample\n",
    "    using the process_sample function, and writes the page images and corresponding\n",
    "    hOCR output to a new shard, one sample per page.\n",
    "\n",
    "    The maxpdfs and maxpages parameters can be used to limit the number of\n",
    "    samples and pages processed. This is useful for testing, as well as limit\n",
    "    the number of pages selected from very long PDF documents.\n",
    "    \"\"\"\n",
    "    with wds.TarWriter(dst) as sink:\n",
    "        for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs):\n",
    "            print(sample[\"__key__\"], sample.keys())\n",
    "            for nsample in process_sample(sample, maxpages=maxpages):\n",
    "                print(\"    \", nsample[\"__key__\"])\n",
    "                sink.write(nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOPEN output.tar {}\n",
      "GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808_00020v6 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1808_00020v6/page-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1808_00020v6/page-01\n",
      "1511_05082v1 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1511_05082v1/page-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1511_05082v1/page-08\n"
     ]
    }
   ],
   "source": [
    "!rm -f output.tar\n",
    "process_shard(shardurls[0], \"output.tar\", maxpdfs=2, maxpages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r--r--r-- bigdata/bigdata 14248 2023-12-17 21:55 1808_00020v6/page-17.hocr\n",
      "-r--r--r-- bigdata/bigdata 277568 2023-12-17 21:55 1808_00020v6/page-17.jpg\n",
      "-r--r--r-- bigdata/bigdata 103527 2023-12-17 21:55 1808_00020v6/page-01.hocr\n",
      "-r--r--r-- bigdata/bigdata 1186871 2023-12-17 21:55 1808_00020v6/page-01.jpg\n",
      "-r--r--r-- bigdata/bigdata   49187 2023-12-17 21:55 1511_05082v1/page-05.hocr\n",
      "-r--r--r-- bigdata/bigdata  589829 2023-12-17 21:55 1511_05082v1/page-05.jpg\n",
      "-r--r--r-- bigdata/bigdata   44814 2023-12-17 21:55 1511_05082v1/page-08.hocr\n",
      "-r--r--r-- bigdata/bigdata  490804 2023-12-17 21:55 1511_05082v1/page-08.jpg\n"
     ]
    }
   ],
   "source": [
    "!tar tvf output.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Processing with Ray\n",
    "\n",
    "This illustrates how to use Ray to process many shards in parallel.\n",
    "\n",
    "You don't need to use Ray for this, you can also invoke `process_shard` in parallel using a job queueing system or using some other distributed computing framework.\n",
    "\n",
    "Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 21:55:57,786\tINFO worker.py:1664 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN /tmp/tmpznj5reiz {}\n",
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 4, block 2\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 27, block 2\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 1, block 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m      1402_1973v2/page-10\n",
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000001.tar {}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m      1402_1973v2/page-09\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1612_01474v3 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m      1808_00020v6/page-08\n",
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m      1808_00020v6/page-19\n",
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m gsutil cp /tmp/tmpezg3f2_5 gs://somebucket/arxiv-pdfs-000001.tar\n",
      "\u001b[36m(process_shard_parallel pid=677113)\u001b[0m      1612_01474v3/page-11\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpdfs = 2  # for testing, we just use two PDFs per shard\n",
    "maxpages = 2  # for testing, we just use two pages per PDF\n",
    "upload_cmd = \"echo gsutil cp {src} {dst}\"  # for testing, we don't actually upload the completed shards\n",
    "\n",
    "import ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "@ray.remote(num_cpus=4)\n",
    "def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999):\n",
    "    \"\"\"Process a shard of the Arxiv dataset and upload the output shard to a bucket.\n",
    "\n",
    "    This function reads a shard of the Arxiv dataset, processes each sample\n",
    "    using the process_sample function, and writes the page images and corresponding \n",
    "    hOCR output to a new shard, one sample per page. The output shard is then\n",
    "    uploaded to the specified bucket using `upload_cmd`.\n",
    "    \"\"\"\n",
    "    dst = dstbucket + \"/\" + os.path.basename(src)\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages)\n",
    "        assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0\n",
    "\n",
    "!rm -f output.tar\n",
    "ray.get([process_shard_parallel.remote(src, \"gs://somebucket\", maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
